{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__萌新瑟瑟发抖，第一次搞NLP的比赛。开始的几天查资料搞出了一个简单的Bert版本，分数0.66左右，调了一阵没提升就没再搞这个版本了，当然那时候的数据构造和文本清洗都非常粗糙，后面也没再尝试一下，转头去开始学习一些经典的NN模型了，相关代码也在下一篇里。__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "阿里的环境跑不了，我是在kaggle跑的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os,re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "from tensorflow.keras import Model, Input, backend as K\n",
    "import transformers #huggingface transformers library\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103505, 5)\n",
      "(10991, 5)\n",
      "(16604, 4)\n",
      "(3011, 3)\n"
     ]
    }
   ],
   "source": [
    "# 加载文本数据\n",
    "TAIL_VALUES = 10\n",
    "\n",
    "sel_data = pd.read_csv('D:/AI/TianCHI/competition_baselines-master/competitions/tianchi_aiops2022/Rawdata//preliminary_sel_log_dataset.csv')\n",
    "sel_data2 = pd.read_csv('D:/AI/TianCHI/competition_baselines-master/competitions/tianchi_aiops2022/Rawdata//preliminary_sel_log_dataset_a.csv')\n",
    "sel_data = pd.concat([sel_data,sel_data2]).sort_values(by=['sn', 'time'], ignore_index=True)\n",
    "sel_data['time'] = pd.to_datetime(sel_data['time'])\n",
    "sel_data['week'] = sel_data['time'].dt.isocalendar().week\n",
    "sel_data = sel_data.groupby(['sn']).tail(TAIL_VALUES)\n",
    "\n",
    "sel_data2.sort_values(by=['sn', 'time'], ignore_index=True, inplace=True)\n",
    "sel_data2['time'] = pd.to_datetime(sel_data2['time'])\n",
    "sel_data2['week'] = sel_data2['time'].dt.isocalendar().week\n",
    "print(sel_data.shape)\n",
    "print(sel_data2.shape)\n",
    "\n",
    "# 加载标签数据\n",
    "train_label = pd.read_csv('D:/AI/TianCHI/competition_baselines-master/competitions/tianchi_aiops2022/Rawdata//preliminary_train_label_dataset.csv')\n",
    "train_label2 = pd.read_csv('D:/AI/TianCHI/competition_baselines-master/competitions/tianchi_aiops2022/Rawdata//preliminary_train_label_dataset_s.csv')\n",
    "train_label = pd.concat([train_label, train_label2]).sort_values(by=['sn', 'fault_time'],ignore_index=True)\n",
    "train_label.drop_duplicates(subset=['sn','fault_time','label'],inplace=True)\n",
    "train_label['fault_time'] = pd.to_datetime(train_label['fault_time'])\n",
    "train_label['week'] = train_label['fault_time'].dt.isocalendar().week\n",
    "print(train_label.shape)\n",
    "\n",
    "# 加载测试数据\n",
    "test_data = pd.read_csv('D:/AI/TianCHI/competition_baselines-master/competitions/tianchi_aiops2022/Rawdata//preliminary_submit_dataset_a.csv')\n",
    "test_data['fault_time'] = pd.to_datetime(test_data['fault_time'])\n",
    "test_data = test_data.sort_values(by=['sn', 'fault_time'],ignore_index=True)\n",
    "test_data['week'] = test_data['fault_time'].dt.isocalendar().week\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    8201\n",
      "1    3234\n",
      "3    2310\n",
      "0    1392\n",
      "Name: label, dtype: int64\n",
      "\n",
      " (3011, 4)\n",
      "Wall time: 2.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 构造训练集\n",
    "train = pd.merge(sel_data, train_label,on=['sn','week'],how='left')\n",
    "train = train.query('time <= fault_time')\n",
    "train.drop_duplicates(subset=['sn','time','msg','fault_time'],inplace=True)\n",
    "train['text'] = train.groupby(['sn','fault_time','week'])['msg'].transform(lambda x : '.'.join(x))\n",
    "train['text'] = train['text'].str.lower()\n",
    "train.drop(['msg','time','week'],axis=1,inplace=True)\n",
    "\n",
    "train.drop_duplicates(subset=['sn','fault_time','text'],inplace=True)\n",
    "train.reset_index(inplace=True,drop = True)\n",
    "train['label'] = train['label'].astype(int)\n",
    "print(train.label.value_counts())\n",
    "\n",
    "# 构造测试集\n",
    "test = pd.merge(test_data, sel_data2,on=['sn','week'],how=\"left\")\n",
    "test = test.query('time <= fault_time')\n",
    "test.drop_duplicates(subset=['sn','time','msg','fault_time'],inplace=True)\n",
    "test['text'] = test.groupby(['sn','fault_time','week'])['msg'].transform(lambda x : '.'.join(x))\n",
    "test['text'] = test['text'].str.lower()\n",
    "test.drop(['msg','time','week'],axis=1,inplace=True)\n",
    "test.drop_duplicates(subset=['sn','fault_time','text'],ignore_index=True,inplace=True)\n",
    "\n",
    "# 补充测试集\n",
    "sn_list = ['05aceb8f3b0f', '0e074cf16c7d', '287cfc92991d', '3845fe1dbef9', '82a187bcfa02', 'b64f4cefbbc6', 'd7ca15888043']\n",
    "supplement_test = test_data.query(\"sn in @sn_list\").copy()\n",
    "supplement_sel_data = sel_data2.query(\"sn in @sn_list\").copy()\n",
    "\n",
    "supplement_test = pd.merge(supplement_test, supplement_sel_data,on=['sn'],how=\"left\")\n",
    "supplement_test['text'] = supplement_test.groupby(['sn','fault_time'])['msg'].transform(lambda x : '.'.join(x))\n",
    "supplement_test['text'] = supplement_test['text'].str.lower()\n",
    "supplement_test.drop(['msg','time','week_x','week_y'], axis=1,inplace=True)\n",
    "supplement_test.drop_duplicates(subset=['sn','fault_time','text'],ignore_index=True,inplace=True)\n",
    "\n",
    "test = pd.concat([test, supplement_test]).sort_values(by=['sn', 'fault_time'],ignore_index=True)\n",
    "test.reset_index(inplace=True,drop = True)\n",
    "print('\\n',test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 sn server_model          fault_time  label  \\\n",
      "0      SERVER_10001         SM57 2020-05-01 10:04:00      1   \n",
      "1      SERVER_10008         SM53 2020-02-25 16:12:00      1   \n",
      "2      SERVER_10008         SM53 2020-03-11 18:04:00      2   \n",
      "3      SERVER_10009         SM53 2020-05-08 16:37:00      3   \n",
      "4      SERVER_10012         SM53 2020-07-13 03:32:00      3   \n",
      "...             ...          ...                 ...    ...   \n",
      "15132   SERVER_9991         SM56 2020-08-04 22:49:00      2   \n",
      "15133   SERVER_9991         SM56 2020-10-07 18:42:00      2   \n",
      "15134   SERVER_9993         SM57 2020-05-14 23:50:00      2   \n",
      "15135   SERVER_9998         SM57 2020-05-29 11:25:00      2   \n",
      "15136   SERVER_9999         SM57 2020-10-13 02:57:00      2   \n",
      "\n",
      "                                                    text  \n",
      "0       processor cpu1_status | ierr | asserted. proc...  \n",
      "1       memory dimm050_stat | uncorrectable ecc | ass...  \n",
      "2       system boot initiated bios_boot_up | initiate...  \n",
      "3       drive slot hdd_l_14_status | drive fault | as...  \n",
      "4       drive slot hdd_l_18_status | drive fault | as...  \n",
      "...                                                  ...  \n",
      "15132   memory cpu0a0_dimm_stat | correctable ecc | a...  \n",
      "15133   memory cpu1a0_dimm_stat | correctable ecc | a...  \n",
      "15134   memory cpu1f0_dimm_stat | correctable ecc | a...  \n",
      "15135   memory cpu1e1_dimm_stat | correctable ecc | a...  \n",
      "15136   memory cpu1c0_dimm_stat | uncorrectable ecc |...  \n",
      "\n",
      "[15137 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                sn          fault_time server_model  \\\n",
      "0     000d33b21436 2020-09-02 16:42:54         SM40   \n",
      "1     005c5a9218ba 2020-06-28 19:05:16         SM99   \n",
      "2     0079283bde6e 2020-04-26 21:32:44         SM14   \n",
      "3     007bdf23b62f 2020-06-16 18:40:39         SM93   \n",
      "4     00a577a8e54f 2020-04-07 07:16:55         SM13   \n",
      "...            ...                 ...          ...   \n",
      "3006  ffbf46b4af21 2019-12-28 20:10:01         SM35   \n",
      "3007  ffc229b6cd9a 2020-06-27 02:39:08         SM49   \n",
      "3008  ffd44698a52b 2020-01-21 15:46:56         SM66   \n",
      "3009  fff73a9e5bd5 2020-03-01 22:43:43         SM92   \n",
      "3010  fffd22fffe19 2020-01-21 19:22:56         SM16   \n",
      "\n",
      "                                                   text  \n",
      "0      system boot initiated bios_boot_up | initiate...  \n",
      "1      memory memory_status | correctable ecc | asse...  \n",
      "2      power supply psu1_supply | failure detected |...  \n",
      "3      memory #0xe2 | correctable ecc | asserted. me...  \n",
      "4      memory mem_chg1_status | correctable ecc | as...  \n",
      "...                                                 ...  \n",
      "3006   memory cpu0c0_dimm_stat | correctable ecc | a...  \n",
      "3007   memory #0xe2 | correctable ecc | asserted. me...  \n",
      "3008   unknown chassis_control |  | asserted. system...  \n",
      "3009   memory #0x87 | correctable ecc | asserted. me...  \n",
      "3010   memory #0xf9 | uncorrectable ecc | asserted. ...  \n",
      "\n",
      "[3011 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             sn          fault_time server_model  \\\n",
      "0  05aceb8f3b0f 2020-03-16 00:41:54         SM23   \n",
      "1  0e074cf16c7d 2020-11-16 00:12:54         SM35   \n",
      "2  287cfc92991d 2020-07-06 00:42:55         SM23   \n",
      "3  3845fe1dbef9 2020-10-26 00:07:54         SM35   \n",
      "4  82a187bcfa02 2020-04-06 00:26:30         SM16   \n",
      "5  b64f4cefbbc6 2020-11-23 03:03:53         SM26   \n",
      "6  d7ca15888043 2020-07-13 01:53:54         SM17   \n",
      "\n",
      "                                                text  \n",
      "0   unknown #0x17 |  | asserted. processor cpu_ca...  \n",
      "1   processor cpu0_status | configuration error |...  \n",
      "2   unknown #0x17 |  | asserted. processor cpu_ca...  \n",
      "3   processor cpu1_status | configuration error |...  \n",
      "4   system acpi power state acpi_pwr_status | s4/...  \n",
      "5   processor cpu0_status |  | asserted. processo...  \n",
      "6   processor cpu_caterr | state asserted | asserted  \n"
     ]
    }
   ],
   "source": [
    "print(supplement_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_two = train.query(\"label == 2\").copy()\n",
    "# train_two = train_two[-4000:].copy()\n",
    "# train_other = train.query(\"label != 2\").copy()\n",
    "# train = pd.concat([train_other,train_two])\n",
    "# train.reset_index(inplace=True,drop = True)\n",
    "# print(train.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_all_entities(text):\n",
    "    text = re.sub('锛�','', text)\n",
    "    text = re.sub('锟絋',' ', text)\n",
    "    text = re.sub('锟�','', text)\n",
    "    text = re.sub('\\|',',', text)\n",
    "    text = re.sub('s4/s5:', 'ss_one_state', text)\n",
    "    text = re.sub('s0/g0:', 'sg_one_state', text)\n",
    "    text = re.sub('s5/g2:', 'sg_two_state', text)\n",
    "    text = re.sub('aa17.{22}','', text)\n",
    "    text = re.sub('000000','a Special tags', text)\n",
    "    text = re.sub('\\d{4}\\w\\d\\w\\d{5}','Asserted oem record', text)\n",
    "    return text\n",
    "train['text'] = train['text'].apply(lambda x: strip_all_entities(x))\n",
    "test['text'] = test['text'].apply(lambda x: strip_all_entities(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus( #batch_encode_plus\n",
    "        texts, \n",
    "        return_attention_mask=False, \n",
    "        return_token_type_ids=False,\n",
    "        padding = 'max_length',\n",
    "        max_length = maxlen,\n",
    "        truncation = True,\n",
    "#         return_tensors = 'tf'\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"D:/AI/TianCHI/tianchi_aiops2022/pretrain/bert-large-uncased/\"  # bert-base-uncased、bert-large-uncased\n",
    "#model_path = \"bert-base-uncased\" \n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]\n",
      "1037\n",
      "a\n",
      "['test', 'batch', 'en', '##code', 'plus']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BertTokenizer' object has no attribute 'batch_encode_plus'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_38684/3012422630.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#['test', 'batch', 'en', '##code', 'plus']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_encode_plus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max_length'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'longest_first'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#长的截，短的补\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BertTokenizer' object has no attribute 'batch_encode_plus'"
     ]
    }
   ],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained('C:\\\\Users\\\\lgy\\\\Desktop\\\\fsdownload\\\\bert-base-uncased')\n",
    "print(tokenizer.mask_token) # [MASK]\n",
    "print(tokenizer.convert_tokens_to_ids('a')) # 1037\n",
    "print(tokenizer.convert_ids_to_tokens(1037)) # a\n",
    " \n",
    "string = \"test batch encode plus\"\n",
    "strings = [string,string]\n",
    "tokens = tokenizer.tokenize(string)\n",
    "print(tokens)#['test', 'batch', 'en', '##code', 'plus']\n",
    "out = tokenizer.batch_encode_plus(strings,max_length=10,padding='max_length',truncation='longest_first')#长的截，短的补\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = train['text']\n",
    "y = train['label']\n",
    "X_train,X_test ,y_train,y_test = train_test_split(X, y, random_state = 2020, test_size = 0.2)\n",
    "max_len = 128\n",
    "Xtrain_encoded = regular_encode(X_train, tokenizer, maxlen=max_len)\n",
    "ytrain_encoded = tf.keras.utils.to_categorical(y_train, num_classes=4,dtype = 'int32')\n",
    "Xtest_encoded = regular_encode(X_test, tokenizer, maxlen=max_len)\n",
    "ytest_encoded = tf.keras.utils.to_categorical(y_test, num_classes=4,dtype = 'int32')\n",
    "\n",
    "Xtest_sub = regular_encode(test['text'], tokenizer, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer, loss='categorical_crossentropy', max_len=512):\n",
    "    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    #adding dropout layer\n",
    "    x = tf.keras.layers.Dropout(0.3)(cls_token)\n",
    "    #using a dense layer of 40 neurons as the number of unique categories is 40. \n",
    "    out = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs=input_word_ids, outputs=out)\n",
    "    #using categorical crossentropy as the loss as it is a multi-class classification problem\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=3e-5), loss=loss, metrics=['accuracy', f1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nTFAutoModel requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://www.tensorflow.org/install and follow the ones that match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32792/3972524962.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# with strategy.scope():\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtransformer_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTFAutoModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#transformer_layer = transformers.from_pretrained(model_path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software\\anaconda\\envs\\gpu\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 860\u001b[1;33m         \u001b[0mrequires_backends\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backends\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software\\anaconda\\envs\\gpu\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m    846\u001b[0m     \u001b[0mfailed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mavailable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchecks\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mavailable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfailed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfailed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: \nTFAutoModel requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://www.tensorflow.org/install and follow the ones that match your environment.\n"
     ]
    }
   ],
   "source": [
    "# with strategy.scope():\n",
    "transformer_layer = transformers.TFAutoModel.from_pretrained(model_path)\n",
    "#transformer_layer = transformers.from_pretrained(model_path)\n",
    "model = build_model(transformer_layer, max_len=max_len)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the training and testing dataset.\n",
    "BATCH_SIZE = 64\n",
    "AUTO = tf.data.experimental.AUTOTUNE \n",
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((Xtrain_encoded, ytrain_encoded))\n",
    "    .repeat()\n",
    "    .shuffle(10000)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    "    )\n",
    "\n",
    "val_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((Xtest_encoded,ytest_encoded))\n",
    "    .shuffle(10000)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    "    )\n",
    "\n",
    "test_sub_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(Xtest_sub)\n",
    "    .batch(BATCH_SIZE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = Xtrain_encoded.shape[0] // BATCH_SIZE\n",
    "train_history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=n_steps,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=18\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_sub_dataset,verbose = 1)\n",
    "pred_classes = np.argmax(preds, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = test[['sn', 'fault_time']].copy()\n",
    "sub['label'] = pred_classes\n",
    "display(sub.head())\n",
    "sub['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('bert_uncase_old.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "tianchi_metadata": {
   "competitions": [],
   "datasets": [
    {
     "id": "121324",
     "title": "获取数据集标题失败"
    }
   ],
   "description": "",
   "notebookId": "340148",
   "source": "dsw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
