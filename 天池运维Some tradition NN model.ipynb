{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "来源：天池比赛论坛上选手的分享\n",
    "__自己一个人蒙头搞了一段时间，基本能想到的都搞了。这版里就用了Focal loss和fast test的embeddings，也没啥新奇的东西。\n",
    "线下embeddings concat有挺大的提升的，但是我没提交试试。现在写好了HAN的版本，线下一般般就没提交看看，还有几个版本的self-attention和Multi-Head attention，但是感觉没啥用，也没去跑。__\n",
    "\n",
    "__一直没有提升搞得我有点崩，接下来试过胶囊网络还没进步就不打算搞了。__\n",
    "\n",
    "__另外刚开始跑bert的时候，就发现0、1很难区分，但是传统的gbdt好像效果还行，简单的CNN连接lgb就能达到我自己Bert模型的效果。___\n",
    "\n",
    "__还有今天肉眼扫了一遍数据集，发现了一些奇怪的数据，就正好更新了一下我的`strip_all_entities`函数,效果怎么样我也没来得及试。__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras \n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "from keras.models import Sequential\n",
    "#from keras import Sequential\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras import layers\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dropout, Bidirectional, Dense\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy, BinaryAccuracy\n",
    "from tensorflow.keras.optimizers import Adamax, Adam\n",
    "from tensorflow.math import rint, reduce_sum, divide, exp, subtract, greater_equal\n",
    "from tensorflow.math import round as rounding\n",
    "from tensorflow.random import set_seed\n",
    "from tensorflow import cast, constant\n",
    "from tensorflow.nn import softmax\n",
    "\n",
    "\n",
    "# !pip install plot-keras-history\n",
    "# from plot_keras_history import plot_history\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score as acu\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "seed = 2022\n",
    "np.random.seed(seed)\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'keras_tensor' from 'keras.engine' (C:\\software\\anaconda\\envs\\gpu\\lib\\site-packages\\keras\\engine\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10712/105757703.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'keras_tensor' from 'keras.engine' (C:\\software\\anaconda\\envs\\gpu\\lib\\site-packages\\keras\\engine\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from keras.engine import keras_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'keras_tensor' from 'keras.engine' (C:\\software\\anaconda\\envs\\gpu\\lib\\site-packages\\keras\\engine\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10712/802459132.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\software\\anaconda\\envs\\gpu\\lib\\site-packages\\tensorflow_addons\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Local project imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software\\anaconda\\envs\\gpu\\lib\\site-packages\\tensorflow_addons\\activations\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;34m\"\"\"Additional activation functions.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgelu\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgelu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhardshrink\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhardshrink\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlisht\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlisht\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software\\anaconda\\envs\\gpu\\lib\\site-packages\\tensorflow_addons\\activations\\gelu.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTensorLike\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software\\anaconda\\envs\\gpu\\lib\\site-packages\\tensorflow_addons\\utils\\types.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# TODO: Remove once https://github.com/tensorflow/tensorflow/issues/44613 is resolved\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;34m\"2.5\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'keras_tensor' from 'keras.engine' (C:\\software\\anaconda\\envs\\gpu\\lib\\site-packages\\keras\\engine\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103505, 5)\n",
      "(10991, 5)\n",
      "(16604, 4)\n",
      "(3011, 3)\n"
     ]
    }
   ],
   "source": [
    "# 加载文本数据\n",
    "TAIL_VALUES = 10\n",
    "\n",
    "sel_data = pd.read_csv('D:/AI/TianCHI/competition_baselines-master/competitions/tianchi_aiops2022/Rawdata/preliminary_sel_log_dataset.csv')\n",
    "sel_data2 = pd.read_csv('D:/AI/TianCHI/competition_baselines-master/competitions/tianchi_aiops2022/Rawdata/preliminary_sel_log_dataset_a.csv')\n",
    "sel_data = pd.concat([sel_data,sel_data2]).sort_values(by=['sn', 'time'], ignore_index=True)\n",
    "sel_data['time'] = pd.to_datetime(sel_data['time'])\n",
    "sel_data['week'] = sel_data['time'].dt.isocalendar().week\n",
    "sel_data = sel_data.groupby(['sn']).tail(TAIL_VALUES)\n",
    "\n",
    "sel_data2.sort_values(by=['sn', 'time'], ignore_index=True, inplace=True)\n",
    "sel_data2['time'] = pd.to_datetime(sel_data2['time'])\n",
    "sel_data2['week'] = sel_data2['time'].dt.isocalendar().week\n",
    "print(sel_data.shape)\n",
    "print(sel_data2.shape)\n",
    "\n",
    "# 加载标签数据\n",
    "train_label = pd.read_csv('D:/AI/TianCHI/competition_baselines-master/competitions/tianchi_aiops2022/Rawdata/preliminary_train_label_dataset.csv')\n",
    "train_label2 = pd.read_csv('D:/AI/TianCHI/competition_baselines-master/competitions/tianchi_aiops2022/Rawdata/preliminary_train_label_dataset_s.csv')\n",
    "train_label = pd.concat([train_label, train_label2]).sort_values(by=['sn', 'fault_time'],ignore_index=True)\n",
    "train_label.drop_duplicates(subset=['sn','fault_time','label'],inplace=True)\n",
    "train_label['fault_time'] = pd.to_datetime(train_label['fault_time'])\n",
    "train_label['week'] = train_label['fault_time'].dt.isocalendar().week\n",
    "print(train_label.shape)\n",
    "\n",
    "# 加载测试数据\n",
    "test_data = pd.read_csv('D:/AI/TianCHI/competition_baselines-master/competitions/tianchi_aiops2022/Rawdata/preliminary_submit_dataset_a.csv')\n",
    "test_data['fault_time'] = pd.to_datetime(test_data['fault_time'])\n",
    "test_data = test_data.sort_values(by=['sn', 'fault_time'],ignore_index=True)\n",
    "test_data['week'] = test_data['fault_time'].dt.isocalendar().week\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    8201\n",
      "1    3234\n",
      "3    2310\n",
      "0    1392\n",
      "Name: label, dtype: int64\n",
      "\n",
      " (3011, 4)\n",
      "Wall time: 2.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 构造训练集\n",
    "train = pd.merge(sel_data, train_label,on=['sn','week'],how='left')\n",
    "train = train.query('time <= fault_time')\n",
    "train.drop_duplicates(subset=['sn','time','msg','fault_time'],inplace=True)\n",
    "train['text'] = train.groupby(['sn','fault_time','week'])['msg'].transform(lambda x : '.'.join(x))\n",
    "train['text'] = train['text'].str.lower()\n",
    "train.drop(['msg','time','week'],axis=1,inplace=True)\n",
    "\n",
    "train.drop_duplicates(subset=['sn','fault_time','text'],inplace=True)\n",
    "train.reset_index(inplace=True,drop = True)\n",
    "train['label'] = train['label'].astype(int)\n",
    "print(train.label.value_counts())\n",
    "\n",
    "# 构造测试集\n",
    "test = pd.merge(test_data, sel_data2,on=['sn','week'],how=\"left\")\n",
    "test = test.query('time <= fault_time')\n",
    "test.drop_duplicates(subset=['sn','time','msg','fault_time'],inplace=True)\n",
    "test['text'] = test.groupby(['sn','fault_time','week'])['msg'].transform(lambda x : '.'.join(x))\n",
    "test['text'] = test['text'].str.lower()\n",
    "test.drop(['msg','time','week'],axis=1,inplace=True)\n",
    "test.drop_duplicates(subset=['sn','fault_time','text'],ignore_index=True,inplace=True)\n",
    "\n",
    "# 补充测试集\n",
    "sn_list = ['05aceb8f3b0f', '0e074cf16c7d', '287cfc92991d', '3845fe1dbef9', '82a187bcfa02', 'b64f4cefbbc6', 'd7ca15888043']\n",
    "supplement_test = test_data.query(\"sn in @sn_list\").copy()\n",
    "supplement_sel_data = sel_data2.query(\"sn in @sn_list\").copy()\n",
    "\n",
    "supplement_test = pd.merge(supplement_test, supplement_sel_data,on=['sn'],how=\"left\")\n",
    "supplement_test['text'] = supplement_test.groupby(['sn','fault_time'])['msg'].transform(lambda x : '.'.join(x))\n",
    "supplement_test['text'] = supplement_test['text'].str.lower()\n",
    "supplement_test.drop(['msg','time','week_x','week_y'], axis=1,inplace=True)\n",
    "supplement_test.drop_duplicates(subset=['sn','fault_time','text'],ignore_index=True,inplace=True)\n",
    "\n",
    "test = pd.concat([test, supplement_test]).sort_values(by=['sn', 'fault_time'],ignore_index=True)\n",
    "test.reset_index(inplace=True,drop = True)\n",
    "print('\\n',test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_two = train.query(\"label == 2\").copy()\n",
    "# train_two = train_two[-4000:].copy()\n",
    "# train_other = train.query(\"label != 2\").copy()\n",
    "# train = pd.concat([train_other,train_two])\n",
    "# train.reset_index(inplace=True,drop = True)\n",
    "# print(train.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_all_entities(text):\n",
    "    text = re.sub('锛�','', text)\n",
    "    text = re.sub('锟絋',' ', text)\n",
    "    text = re.sub('锟�','', text)\n",
    "    text = re.sub('\\|',',', text)\n",
    "    text = re.sub('s4/s5:', 'ss_one_state', text)\n",
    "    text = re.sub('s0/g0:', 'sg_one_state', text)\n",
    "    text = re.sub('s5/g2:', 'sg_two_state', text)\n",
    "    text = re.sub('aa17.{22}','', text)\n",
    "    text = re.sub('000000','a Special tags', text)\n",
    "    text = re.sub('\\d{4}\\w\\d\\w\\d{5}','Asserted oem record', text)\n",
    "    return text\n",
    "train['text'] = train['text'].apply(lambda x: strip_all_entities(x))\n",
    "test['text'] = test['text'].apply(lambda x: strip_all_entities(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 6548, 1: 2598, 3: 1846, 0: 1117})\n",
      "Padded and Tokenized Training Sequence (12109, 110)\n",
      "Target Training Values Shape (12109, 4)\n",
      "_____________________________________________\n",
      "Padded and Tokenized Validation Sequence (3028, 110)\n",
      "Target Validatation Values Shape (3028, 4)\n"
     ]
    }
   ],
   "source": [
    "X = train['text']\n",
    "y = train['label']\n",
    "\n",
    "# 欠采样 -------------------------------------------\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# undersample = RandomUnderSampler(sampling_strategy={2:2398,1:3380,3:2398,0:1475})\n",
    "# X_train, y_train = undersample.fit_resample(np.array(X).reshape(-1, 1), np.array(y).reshape(-1, 1))\n",
    "# train_os = pd.DataFrame(list(zip([x[0] for x in X_train], y_train)), columns = ['text', 'label'])\n",
    "# X = train_os['text'].values\n",
    "# y = train_os['label'].values\n",
    "# -------------------------------------------------\n",
    "\n",
    "NUM_CLASSES = 4\n",
    "MAX_LEN = 110\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "sentences_train,sentences_val,y_train,y_val = train_test_split(X,y,test_size=0.2,random_state=11) # ,stratify = train_all['label']\n",
    "print(Counter(y_train))\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes = NUM_CLASSES)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes = NUM_CLASSES)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_val = tokenizer.texts_to_sequences(sentences_val)\n",
    "\n",
    "# Adding 1 because of  reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1 # (in case of pre-trained embeddings it's +2)                         \n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=MAX_LEN)\n",
    "X_val = pad_sequences(X_val, padding='post', maxlen=MAX_LEN)\n",
    "\n",
    "\n",
    "print(\"Padded and Tokenized Training Sequence\".format(),X_train.shape)\n",
    "print(\"Target Training Values Shape\".format(),y_train.shape)\n",
    "print(\"_____________________________________________\")\n",
    "print(\"Padded and Tokenized Validation Sequence\".format(),X_val.shape)\n",
    "print(\"Target Validatation Values Shape\".format(),y_val.shape)\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(test['text'])\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fasttext embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15137\n",
      "26786\n"
     ]
    }
   ],
   "source": [
    "# 额外的语料库\n",
    "addition_corpus = pd.read_csv('D:/AI/TianCHI/competition_baselines-master/competitions/tianchi_aiops2022/Rawdata/additional_sel_log_dataset.csv')\n",
    "addition_corpus = addition_corpus.sort_values(by=['sn', 'time'], ignore_index=True)\n",
    "addition_corpus = addition_corpus.groupby(['sn']).tail(9)\n",
    "addition_corpus['text'] = addition_corpus.groupby(['sn'])['msg'].transform(lambda x : ''.join(x))\n",
    "addition_corpus['text'] = addition_corpus['text'].str.lower()\n",
    "addition_corpus.drop(['msg'],axis=1,inplace=True)\n",
    "addition_corpus.drop_duplicates(subset=['sn', 'text'],ignore_index=True,inplace=True)\n",
    "\n",
    "training_corpus = train['text']   # train_all\n",
    "print(len(training_corpus))\n",
    "training_corpus = training_corpus.append(addition_corpus['text'],ignore_index=True)\n",
    "print(len(training_corpus))\n",
    "training_corpus = pd.DataFrame(training_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from textblob import TextBlob\n",
    "corpus = []\n",
    "for row in zip(*training_corpus.to_dict(\"list\").values()):\n",
    "    zen = TextBlob(row[0]).words\n",
    "    corpus.append(zen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 11:26:01,813 : INFO : collecting all words and their counts\n",
      "2022-03-31 11:26:01,814 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-03-31 11:26:01,868 : INFO : PROGRESS: at sentence #10000, processed 393643 words, keeping 923 word types\n",
      "2022-03-31 11:26:01,938 : INFO : PROGRESS: at sentence #20000, processed 887111 words, keeping 1333 word types\n",
      "2022-03-31 11:26:01,994 : INFO : collected 2612 word types from a corpus of 1300406 raw words and 26786 sentences\n",
      "2022-03-31 11:26:01,995 : INFO : Creating a fresh vocabulary\n",
      "2022-03-31 11:26:02,012 : INFO : FastText lifecycle event {'msg': 'effective_min_count=1 retains 2612 unique words (100.0%% of original 2612, drops 0)', 'datetime': '2022-03-31T11:26:02.012610', 'gensim': '4.0.1', 'python': '3.7.9 (default, Aug 31 2020, 17:10:11) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2022-03-31 11:26:02,013 : INFO : FastText lifecycle event {'msg': 'effective_min_count=1 leaves 1300406 word corpus (100.0%% of original 1300406, drops 0)', 'datetime': '2022-03-31T11:26:02.013611', 'gensim': '4.0.1', 'python': '3.7.9 (default, Aug 31 2020, 17:10:11) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2022-03-31 11:26:02,026 : INFO : deleting the raw counts dictionary of 2612 items\n",
      "2022-03-31 11:26:02,027 : INFO : sample=0.001 downsamples 62 most-common words\n",
      "2022-03-31 11:26:02,028 : INFO : FastText lifecycle event {'msg': 'downsampling leaves estimated 548230.2324587359 word corpus (42.2%% of prior 1300406)', 'datetime': '2022-03-31T11:26:02.028612', 'gensim': '4.0.1', 'python': '3.7.9 (default, Aug 31 2020, 17:10:11) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2022-03-31 11:26:02,030 : INFO : constructing a huffman tree from 2612 words\n",
      "2022-03-31 11:26:02,097 : INFO : built huffman tree with maximum node depth 21\n",
      "2022-03-31 11:26:02,135 : INFO : estimated required memory for 2612 words, 2000000 buckets and 200 dimensions: 1608590036 bytes\n",
      "2022-03-31 11:26:02,135 : INFO : resetting layer weights\n",
      "2022-03-31 11:26:03,543 : INFO : FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-03-31T11:26:03.543532', 'gensim': '4.0.1', 'python': '3.7.9 (default, Aug 31 2020, 17:10:11) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n",
      "2022-03-31 11:26:03,543 : INFO : FastText lifecycle event {'msg': 'training model with 3 workers on 2612 vocabulary and 200 features, using sg=1 hs=1 sample=0.001 negative=5 window=8', 'datetime': '2022-03-31T11:26:03.543532', 'gensim': '4.0.1', 'python': '3.7.9 (default, Aug 31 2020, 17:10:11) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2022-03-31 11:26:04,596 : INFO : EPOCH 1 - PROGRESS: at 18.36% examples, 74537 words/s, in_qsize 6, out_qsize 1\n",
      "2022-03-31 11:26:05,733 : INFO : EPOCH 1 - PROGRESS: at 37.98% examples, 78949 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:26:06,764 : INFO : EPOCH 1 - PROGRESS: at 54.08% examples, 78798 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:07,774 : INFO : EPOCH 1 - PROGRESS: at 68.07% examples, 80667 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:08,868 : INFO : EPOCH 1 - PROGRESS: at 80.51% examples, 79952 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:09,885 : INFO : EPOCH 1 - PROGRESS: at 95.06% examples, 81336 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:10,182 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-31 11:26:10,206 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-31 11:26:10,250 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-31 11:26:10,250 : INFO : EPOCH - 1 : training on 1300406 raw words (547898 effective words) took 6.7s, 81744 effective words/s\n",
      "2022-03-31 11:26:11,288 : INFO : EPOCH 2 - PROGRESS: at 18.36% examples, 76025 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:26:12,416 : INFO : EPOCH 2 - PROGRESS: at 37.98% examples, 80277 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:13,450 : INFO : EPOCH 2 - PROGRESS: at 54.08% examples, 79650 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:26:14,457 : INFO : EPOCH 2 - PROGRESS: at 68.07% examples, 81309 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:15,478 : INFO : EPOCH 2 - PROGRESS: at 80.74% examples, 81295 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:16,502 : INFO : EPOCH 2 - PROGRESS: at 95.06% examples, 82667 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:26:16,786 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-31 11:26:16,819 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-31 11:26:16,855 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-31 11:26:16,856 : INFO : EPOCH - 2 : training on 1300406 raw words (548904 effective words) took 6.6s, 83134 effective words/s\n",
      "2022-03-31 11:26:17,921 : INFO : EPOCH 3 - PROGRESS: at 18.64% examples, 73637 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:19,050 : INFO : EPOCH 3 - PROGRESS: at 37.79% examples, 79344 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:26:20,069 : INFO : EPOCH 3 - PROGRESS: at 54.08% examples, 79148 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:26:21,123 : INFO : EPOCH 3 - PROGRESS: at 68.67% examples, 80977 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:26:22,132 : INFO : EPOCH 3 - PROGRESS: at 80.74% examples, 80476 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:23,168 : INFO : EPOCH 3 - PROGRESS: at 94.41% examples, 81132 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:23,511 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-31 11:26:23,543 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-31 11:26:23,544 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-31 11:26:23,544 : INFO : EPOCH - 3 : training on 1300406 raw words (548338 effective words) took 6.7s, 82028 effective words/s\n",
      "2022-03-31 11:26:24,588 : INFO : EPOCH 4 - PROGRESS: at 18.64% examples, 74790 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:26:25,597 : INFO : EPOCH 4 - PROGRESS: at 36.13% examples, 79850 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:26,611 : INFO : EPOCH 4 - PROGRESS: at 51.34% examples, 78388 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:27,612 : INFO : EPOCH 4 - PROGRESS: at 65.18% examples, 79598 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:28,684 : INFO : EPOCH 4 - PROGRESS: at 78.35% examples, 79618 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:26:29,732 : INFO : EPOCH 4 - PROGRESS: at 92.00% examples, 80216 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:26:30,224 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-31 11:26:30,245 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-31 11:26:30,297 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-31 11:26:30,298 : INFO : EPOCH - 4 : training on 1300406 raw words (548617 effective words) took 6.8s, 81271 effective words/s\n",
      "2022-03-31 11:26:31,350 : INFO : EPOCH 5 - PROGRESS: at 18.36% examples, 74925 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:32,498 : INFO : EPOCH 5 - PROGRESS: at 37.79% examples, 79134 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:33,514 : INFO : EPOCH 5 - PROGRESS: at 54.08% examples, 79098 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:34,538 : INFO : EPOCH 5 - PROGRESS: at 68.67% examples, 81450 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:35,577 : INFO : EPOCH 5 - PROGRESS: at 81.10% examples, 81420 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:26:36,637 : INFO : EPOCH 5 - PROGRESS: at 95.83% examples, 82149 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:36,877 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-31 11:26:36,882 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-31 11:26:36,947 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-31 11:26:36,947 : INFO : EPOCH - 5 : training on 1300406 raw words (548379 effective words) took 6.6s, 82511 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 11:26:38,012 : INFO : EPOCH 6 - PROGRESS: at 18.11% examples, 74259 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:39,030 : INFO : EPOCH 6 - PROGRESS: at 36.10% examples, 79250 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:40,103 : INFO : EPOCH 6 - PROGRESS: at 51.34% examples, 76177 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:41,123 : INFO : EPOCH 6 - PROGRESS: at 65.18% examples, 77568 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:42,238 : INFO : EPOCH 6 - PROGRESS: at 78.35% examples, 77285 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:43,293 : INFO : EPOCH 6 - PROGRESS: at 92.00% examples, 78228 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:26:43,805 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-31 11:26:43,848 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-31 11:26:43,873 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-31 11:26:43,874 : INFO : EPOCH - 6 : training on 1300406 raw words (548862 effective words) took 6.9s, 79277 effective words/s\n",
      "2022-03-31 11:26:44,895 : INFO : EPOCH 7 - PROGRESS: at 17.34% examples, 73171 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:26:45,946 : INFO : EPOCH 7 - PROGRESS: at 35.24% examples, 77151 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:46,983 : INFO : EPOCH 7 - PROGRESS: at 51.34% examples, 77267 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:48,027 : INFO : EPOCH 7 - PROGRESS: at 65.75% examples, 78845 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:26:49,041 : INFO : EPOCH 7 - PROGRESS: at 78.97% examples, 79932 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:26:50,086 : INFO : EPOCH 7 - PROGRESS: at 92.00% examples, 79870 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:26:50,585 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-31 11:26:50,613 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-31 11:26:50,658 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-31 11:26:50,658 : INFO : EPOCH - 7 : training on 1300406 raw words (548229 effective words) took 6.8s, 80847 effective words/s\n",
      "2022-03-31 11:26:51,720 : INFO : EPOCH 8 - PROGRESS: at 18.11% examples, 74460 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:26:52,729 : INFO : EPOCH 8 - PROGRESS: at 36.99% examples, 81840 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:26:53,783 : INFO : EPOCH 8 - PROGRESS: at 52.18% examples, 78644 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:54,785 : INFO : EPOCH 8 - PROGRESS: at 66.35% examples, 80379 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:26:55,828 : INFO : EPOCH 8 - PROGRESS: at 79.58% examples, 80805 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:26:56,864 : INFO : EPOCH 8 - PROGRESS: at 93.20% examples, 81378 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:57,284 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-31 11:26:57,334 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-31 11:26:57,344 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-31 11:26:57,344 : INFO : EPOCH - 8 : training on 1300406 raw words (548681 effective words) took 6.7s, 82107 effective words/s\n",
      "2022-03-31 11:26:58,416 : INFO : EPOCH 9 - PROGRESS: at 18.64% examples, 73030 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:26:59,566 : INFO : EPOCH 9 - PROGRESS: at 37.79% examples, 78428 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:27:00,606 : INFO : EPOCH 9 - PROGRESS: at 54.08% examples, 77953 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:27:01,615 : INFO : EPOCH 9 - PROGRESS: at 67.48% examples, 79205 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:27:02,645 : INFO : EPOCH 9 - PROGRESS: at 80.15% examples, 79463 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:27:03,660 : INFO : EPOCH 9 - PROGRESS: at 93.20% examples, 79880 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:27:04,091 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-31 11:27:04,141 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-31 11:27:04,160 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-31 11:27:04,160 : INFO : EPOCH - 9 : training on 1300406 raw words (548221 effective words) took 6.8s, 80472 effective words/s\n",
      "2022-03-31 11:27:05,235 : INFO : EPOCH 10 - PROGRESS: at 18.64% examples, 72567 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:27:06,379 : INFO : EPOCH 10 - PROGRESS: at 37.98% examples, 78048 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:27:07,397 : INFO : EPOCH 10 - PROGRESS: at 54.08% examples, 78584 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:27:08,426 : INFO : EPOCH 10 - PROGRESS: at 68.67% examples, 80983 words/s, in_qsize 6, out_qsize 0\n",
      "2022-03-31 11:27:09,435 : INFO : EPOCH 10 - PROGRESS: at 80.74% examples, 80597 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:27:10,445 : INFO : EPOCH 10 - PROGRESS: at 93.73% examples, 80869 words/s, in_qsize 5, out_qsize 0\n",
      "2022-03-31 11:27:10,809 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-03-31 11:27:10,827 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-03-31 11:27:10,891 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-03-31 11:27:10,891 : INFO : EPOCH - 10 : training on 1300406 raw words (548426 effective words) took 6.7s, 81518 effective words/s\n",
      "2022-03-31 11:27:10,892 : INFO : FastText lifecycle event {'msg': 'training on 13004060 raw words (5484555 effective words) took 67.3s, 81436 effective words/s', 'datetime': '2022-03-31T11:27:10.892587', 'gensim': '4.0.1', 'python': '3.7.9 (default, Aug 31 2020, 17:10:11) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2022-03-31 11:27:10,984 : INFO : FastText lifecycle event {'params': 'FastText(vocab=2612, vector_size=200, alpha=0.025)', 'datetime': '2022-03-31T11:27:10.984590', 'gensim': '4.0.1', 'python': '3.7.9 (default, Aug 31 2020, 17:10:11) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import FastText\n",
    "# model = FastText(vector_size=200, window=8, min_count=0)  # instantiate\n",
    "# model.build_vocab(corpus_iterable = corpus)\n",
    "# model.train(corpus_iterable=corpus, total_examples=len(a), epochs=10)\n",
    "model_fasttext = FastText(vector_size=EMBEDDING_DIM, window=8, min_count=1, sentences=corpus, epochs=10,sg = 1,hs = 1,sample = 1e-3, ns_exponent = 0.25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_embedding_idx = {}\n",
    "for index, word in enumerate(model_fasttext.wv.index_to_key):\n",
    "    embeddings = np.asarray(model_fasttext.wv[word])\n",
    "    fasttext_embedding_idx[word] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "655 tokens in vocab, 182 tokens out of vocab\n"
     ]
    }
   ],
   "source": [
    "iv = 0\n",
    "oov = 0\n",
    "\n",
    "embedding_idx = fasttext_embedding_idx # swap between embeddings\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embedding_idx.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        iv += 1\n",
    "        # words not found in the embedding space are all zeros\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else: \n",
    "        oov += 1\n",
    "        \n",
    "print('%i tokens in vocab, %i tokens out of vocab' % (iv, oov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    vocab_size,\n",
    "    EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adamax(learning_rate=0.002, decay=0.0005, clipvalue=10)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "metrics=[\n",
    "       tf.keras.metrics.Precision(),\n",
    "       tf.keras.metrics.Recall(),\n",
    "       tfa.metrics.F1Score(num_classes=NUM_CLASSES,average='macro',threshold=0.5)]\n",
    "\n",
    "def FocalLoss(target, input):\n",
    "    gamma = 3. # Hyperparameter that can be tuned.\n",
    "\n",
    "    input = tf.cast(input, tf.float32)    \n",
    "    max_val = K.clip(-input, 0, 1)\n",
    "    loss = input - input * target + max_val + K.log(K.exp(-max_val) + K.exp(-input - max_val))\n",
    "    invprobs = tf.math.log_sigmoid(-input * (target * 2.0 - 1.0))\n",
    "    loss = K.exp(invprobs * gamma) * loss    \n",
    "    return K.mean(K.sum(loss, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.layers' has no attribute 'Conv1D'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_43100/2115272753.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0membedded_sequences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint_sequences_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdrop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded_sequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.layers' has no attribute 'Conv1D'"
     ]
    }
   ],
   "source": [
    "int_sequences_input = Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "drop = Dropout(0.3)(embedded_sequences)\n",
    "x = layers.Conv1D(256, 5, activation=\"relu\")(drop)\n",
    "x = layers.MaxPooling1D()(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling1D()(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "x = layers.Dense(64,activation='relu')(x)\n",
    "preds = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "model = Model(int_sequences_input, preds)\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=optimizer,loss=FocalLoss, metrics=metrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(X_train, y_train,epochs=10, validation_data=(X_val, y_val),batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test,verbose = 1)\n",
    "pred_classes = np.argmax(preds, axis = 1)\n",
    "print(Counter(pred_classes))\n",
    "submit = test.copy()\n",
    "submit['label'] = pred_classes\n",
    "submit['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "def model_cnn():\n",
    "    filter_sizes = [1,2,3,5]\n",
    "    num_filters = 36\n",
    "\n",
    "    inp = Input(shape=(MAX_LEN,))\n",
    "    x = embedding_layer(inp)\n",
    "    x = Reshape((MAX_LEN, EMBEDDING_DIM, 1))(x)\n",
    "\n",
    "    maxpool_pool = []\n",
    "    for i in range(len(filter_sizes)):\n",
    "        conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], EMBEDDING_DIM),\n",
    "                                     kernel_initializer='he_normal', activation='relu')(x)\n",
    "        maxpool_pool.append(MaxPool2D(pool_size=(MAX_LEN - filter_sizes[i] + 1, 1))(conv))\n",
    "\n",
    "    z = Concatenate(axis=1)(maxpool_pool)   \n",
    "    z = Flatten()(z)\n",
    "    z = Dropout(0.1)(z)\n",
    "\n",
    "    outp = Dense(3, activation=\"softmax\")(z)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss=FocalLoss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    return model\n",
    "model = model_cnn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)\n",
    "for e in range(8):\n",
    "    model.fit(X_train, y_train, batch_size=128, epochs=1, validation_data=(X_val, y_val), \n",
    "              verbose = 1, callbacks = [early_stop])\n",
    "pred_val_y = model.predict([X_val], batch_size=128, verbose=0)\n",
    "pred_test_y = model.predict([X_test], batch_size=128, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_classes = np.argmax(pred_test_y, axis = 1)\n",
    "submit = test.copy()\n",
    "submit['label'] = pred_classes\n",
    "submit['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-GRU + Bi-LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as K\n",
    "# from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.layers import BatchNormalization, Conv1D, MaxPooling1D\n",
    "\n",
    "# check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "#                               save_best_only = True, mode = \"min\")\n",
    "# ra_val = RocAucEvaluation(validation_data=(X_valid, Y_valid), interval = 1)\n",
    "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)\n",
    "\n",
    "def build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n",
    "    inp = Input(shape = (MAX_LEN,))\n",
    "    x = embedding_layer(inp)\n",
    "    x1 = SpatialDropout1D(dr)(x)\n",
    "\n",
    "    x = Bidirectional(GRU(units, return_sequences = True))(x1)\n",
    "    x = Conv1D(int(units/2), kernel_size = 7, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "    \n",
    "    y = Bidirectional(LSTM(units, return_sequences = True))(x1)\n",
    "    y = Conv1D(int(units/2), kernel_size = 7, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n",
    "    \n",
    "    avg_pool1 = GlobalAveragePooling1D()(x)\n",
    "    max_pool1 = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    avg_pool2 = GlobalAveragePooling1D()(y)\n",
    "    max_pool2 = GlobalMaxPooling1D()(y)\n",
    "    \n",
    "    \n",
    "    x = concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])\n",
    "\n",
    "    x = Dense(NUM_CLASSES, activation = \"softmax\")(x)\n",
    "    model = Model(inputs = inp, outputs = x)\n",
    "    model.summary()\n",
    "    model.compile(loss = FocalLoss, optimizer = Adamax(learning_rate = lr, decay = lr_d), metrics = metrics)\n",
    "    history = model.fit(X_train, y_train, batch_size = 128, epochs = 30, validation_data = (X_val, y_val), \n",
    "                        verbose = 1, callbacks = [ early_stop])\n",
    "    return model\n",
    "    \n",
    "model = build_model(lr = 2e-3, lr_d = 0.0005, units = 128, dr = 0.2)\n",
    "preds = model.predict(X_test, batch_size = 64, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_classes = np.argmax(preds, axis = 1)\n",
    "submit = test.copy()\n",
    "submit['label'] = pred_classes\n",
    "submit['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "tianchi_metadata": {
   "competitions": [],
   "datasets": [
    {
     "id": "121324",
     "title": "获取数据集标题失败"
    }
   ],
   "description": "",
   "notebookId": "340174",
   "source": "dsw"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
